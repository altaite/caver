/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */
package algorithms.clustering;

import algorithms.clustering.layers.LayeredTunnel;
import algorithms.clustering.layers.LayeredTunnels;
import caver.CalculationSettings;
import caver.Printer;
import caver.ui.CalculationException;
import chemistry.pdb.SnapId;
import geometry.primitives.Point;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.SortedSet;
import java.util.StringTokenizer;
import java.util.TreeSet;
import java.util.logging.Level;
import java.util.logging.Logger;
import tunnels.Tunnel;
import tunnels.TunnelCostComparator;
import upgma.MemoryConstrainedAverageLinkClustering;
import weka.classifiers.Classifier;
import weka.classifiers.Evaluation;
import weka.core.Attribute;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.converters.ConverterUtils.DataSource;

/**
 *
 * @author petr
 * @author tonda
 *
 * The class does hierarchical clustering of tunnels and produces outputs.
 * If connected to GUI, all operations with clusters should be implemented here.
 *
 */
public class Clustering {

    Point source_;
    private List<Tunnel> tunnels_;
    //private LayeredTunnels layered_;
    private CalculationSettings cs_;
    //private static final String sep = ", ";
    public static final String[] colors = {"red", "green", "blue", "cyan",
        "magenta", "yellow", "brown", "brightorange",
        "lightteal", "limon", "darksalmon", "lime", "purpleblue"};
    LayeredTunnels layered_;

    public Clustering(Point source, Collection<Tunnel> tunnels,
            CalculationSettings settings) throws CalculationException {
        source_ = source;
        cs_ = settings;
        tunnels_ = new ArrayList<Tunnel>();
        for (Tunnel t : tunnels) {
            //if (settings.getProbeRadius() <= t.getBottleneckRadius()) {
            this.tunnels_.add(t);
            //}
        }
//        if (this.tunnels_.isEmpty()) {
//            CaverLog.warning("No tunnels left after dumping "
//                    + "tunnels with bottleneck <"
//                    + settings.getProbeRadius() + ".");
//        }
//        System.out.println("Loaded " + tunnels.size() + " tunnels.");
//        System.out.println("Bottleneck radius filter ("
//                + settings.getProbeRadius()
//                + ") survived " + tunnels_.size() + " tunnels.");


        if (tunnels_.isEmpty()) {
            Logger.getLogger("caver").log(Level.WARNING,
                    "No tunnels in cluster .");
        }

    }

    /*
     * Returns true if average distance of random tunnel subsample of
     * at least one cluster is bigger than cutoff value.
     * Makes cutoff value comparable as in average link, even though
     * single link clustering disadvantages (long links) remains.
     */
    private boolean hasBigClusters(Clusters clusters) {


        for (Cluster c : clusters.getClusters()) {

            List<Tunnel> tunnels = c.getRandomTunnels(
                    cs_.getOptimizationSampleSize(), cs_.getRandom());
            LayeredTunnels layered = new LayeredTunnels(source_, tunnels, cs_);

            double sum = 0;
            int count = 0;

            for (int x = 0; x < layered.size(); x++) {
                for (int y = 0; y < x; y++) {
                    sum += layered.getDistance(x, y);
                    count++;
                }
            }

            if (cs_.getDistanceThreshold() < sum / count) {
                return true;
            }
        }
        return false;
    }

    public Clusters optimizedSingleLinkageHierarchicalClustering(
            File file) throws IOException {

        double level;
        Clusters result = new Clusters();
        double up = 100;
        double down = 0.01f;
        level = 4;
        for (int i = 0; i < 10; i++) {
            result = singleLinkageHierarchicalClustering(level, file);
            if (hasBigClusters(result)) {
                up = level;
            } else {
                down = level;
            }
            level = (up + down) / 2;
        }
        return result;
    }

    public Clusters singleLinkageHierarchicalClustering(
            double level, File file) throws IOException {
        Clusters result = new Clusters();
        if (tunnels_.isEmpty()) {
        } else if (1 == tunnels_.size()) {
            Tunnel t = tunnels_.iterator().next();
            result.addTunnel(ClusterId.create(1), t);
        } else {
            LayeredTunnels layered = new LayeredTunnels(source_, tunnels_, cs_);
            Matrix m = layered;
            SLink s;
            if (null != file) {
                s = new SLink(m, file);
            } else {
                s = new SLink(m);
            }
            s.setVerbosity(true);
            s.run();
            List<Integer> clusters = new ArrayList<Integer>();
            s.cut(level, clusters);

            int index = 0;
            for (int cluster : clusters) {
                for (int element : s.getElements(cluster)) {
                    result.addTunnel(ClusterId.create(index), tunnels_.get(element));
                }
                index++;
            }
        }
        return result;
    }

    public SortedSet<Tunnel> bestStaysClustering() throws IOException {

        Map<Tunnel, Integer> indeces = new HashMap<Tunnel, Integer>();
        for (int i = 0; i < tunnels_.size(); i++) {
            indeces.put(tunnels_.get(i), i);
        }

        LayeredTunnels layered = new LayeredTunnels(source_, tunnels_, cs_);

        List<Tunnel> sorted = new ArrayList<Tunnel>();
        sorted.addAll(tunnels_);
        Collections.sort(sorted, new TunnelCostComparator());

        SortedSet<Tunnel> cores = new TreeSet<Tunnel>();

        while (!sorted.isEmpty()) {

            Tunnel core = sorted.remove(0);
            cores.add(core);
            int a = indeces.get(core);

            //System.out.println(core.getCost() + " $");
            List<Integer> redundant = new ArrayList<Integer>();
            for (int i = 0; i < sorted.size(); i++) {
                Tunnel t = sorted.get(i);
                int b = indeces.get(t);
                float d = layered.getDistance(a, b);
                if (d < cs_.getFrameDistanceThreshold()) {
                    //System.out.println("removing " + t.getCost() + " $ at dist " + d);
                    redundant.add(i);
                }
            }
            for (int i = redundant.size() - 1; 0 <= i; i--) {
                int r = redundant.get(i);
                sorted.remove(r);
            }

        }
        return cores;
    }


    /* Loads tree in the MCUPGMA program file format.
     * Efﬁcient algorithms for accurate hierarchical clustering of huge
     * datasets: tackling the entire protein space
     */
    public Clusters loadTree(File treeFile, double cut) throws IOException {
        Clusters clusters = new Clusters();
        BufferedReader br = new BufferedReader(new FileReader(treeFile));
        String line;
        double dist = 0;

        for (int i = 0; i < tunnels_.size(); i++) {
            clusters.addTunnel(ClusterId.create(i), tunnels_.get(i));
        }
        while (null != (line = br.readLine())
                && dist < cut) {
            if (line.trim().length() == 0) {
                continue;
            }
            StringTokenizer st = new StringTokenizer(line, "\t ;");
            ClusterId[] childs = new ClusterId[2];
            for (int i = 0; i < 2; i++) {
                childs[i] = ClusterId.create(
                        Integer.parseInt(st.nextToken()) - 1);
            }
            dist = Double.parseDouble(st.nextToken());
            if (dist < cut) {
                ClusterId parent =
                        ClusterId.create(Integer.parseInt(st.nextToken()) - 1);
                clusters.merge(childs, parent);
            }
        }
        return clusters;
    }

    public int getLayersCount() {
        return layered_.getLayersCount();
    }

    public void computeMatrix(
            File matrixFile) throws IOException {
        Printer.println("Calculating distance matrix");

        layered_ = new LayeredTunnels(source_, tunnels_, cs_);

        BufferedWriter bw = new BufferedWriter(
                new FileWriter(matrixFile));
        for (int x = 0; x < layered_.size(); x++) {
            for (int y = 0; y < layered_.size(); y++) {
                if (x < y) {
                    float d = layered_.getDistance(x, y);
                    bw.write((x + 1) + "\t" + (y + 1) + "\t" + d + "\n");
                }
            }
        }
        bw.close();
    }

    public void memoryConstrainedAverageLink(
            File matrixFile,
            File clusteringTemporaryFile,
            File treeFile)
            throws IOException {

        MemoryConstrainedAverageLinkClustering m =
                new MemoryConstrainedAverageLinkClustering();
        m.run(matrixFile, clusteringTemporaryFile, treeFile);
    }

//    public void naiveAverageLink(
//            File matrixFile,
//
//            File treeFile)
//            throws IOException {
//
//        MemoryConstrainedAverageLinkClustering m =
//                new MemoryConstrainedAverageLinkClustering();
//        m.run(matrixFile, clusteringTemporaryFile, treeFile);
//    }
    public void saveTrainingData(Clusters clusters, File file,
            SortedSet<SnapId> snaps)
            throws IOException {

        if (null == layered_) {
            layered_ = new LayeredTunnels(source_, tunnels_, cs_);
        }

        clusters.calculateStatistics(snaps,
                cs_.getThroughputBestFraction());
        clusters.computePriorities();

        int N = cs_.getMinTrainingTunnels();

        Set<Cluster> suitable = new HashSet<Cluster>();
        for (Cluster c : clusters.getClusters()) {
            if (N <= c.size() && c.getPriority() <= cs_.getMaxTrainingClusters()) {
                suitable.add(c);
            }
        }

        String ids = "";
        for (Cluster c : suitable) {
            ids += c.getId() + ",";
        }
        ids += "X";

        BufferedWriter bw = new BufferedWriter(new FileWriter(file));
        bw.write("@relation tunnels\n");
        for (int i = 0; i < layered_.getLayersCount(); i++) {
            bw.write("@attribute X" + i + " numeric\n");
            bw.write("@attribute Y" + i + " numeric\n");
            bw.write("@attribute Z" + i + " numeric\n");
        }

        bw.write("@attribute length numeric\n");

        bw.write("@attribute cluster {" + ids + "}\n");
        bw.write("@data\n");
        for (LayeredTunnel lt : layered_) {
            Tunnel t = lt.getTunnel();
            Cluster c = t.getCluster();

            if (lt.size() != layered_.getLayersCount()) {
                Logger.getLogger("caver").log(Level.WARNING,
                        "ARFF: layeres number not constant.");
            }

            for (int i = 0; i < lt.size(); i++) {
                bw.write(lt.getX(i) + "," + lt.getY(i) + "," + lt.getZ(i) + ",");
            }
            bw.write(lt.getTunnel().getLength() + ",");

            if (suitable.contains(c)) {
                bw.write(c.getId().get() + "\n");
            } else {
                bw.write("X\n");
            }

        }
        bw.close();
    }

    private Instance describe(LayeredTunnel lt) {

        int n = lt.size() * 3 + 1;
        Instance i = new Instance(n);


        return i;
    }

//        Instances train = new Instances(trainIn);
//        Instances test = new Instances(testIn);
//        train.deleteStringAttributes();
//        test.deleteStringAttributes();
//        classifier.buildClassifier(train);
//        Evaluation eval = new Evaluation(train);
//        eval.evaluateModel(classifier, test);
    public static Instances load(File in) throws IOException {
        try {
            DataSource source = new DataSource(in.getPath());
            Instances inst = source.getDataSet();
            if (inst.classIndex() == -1) {
                inst.setClassIndex(inst.numAttributes() - 1);
            }
            return inst;
        } catch (Exception e) {
            throw new IOException(e);
        }
    }

    public static String getValue(Instances inst, int i, int a) {
        Attribute A = inst.attribute(a);
        double value = inst.instance(i).value(A);
        int index = (int) Math.round(value);
        return inst.attribute(a).value(index);
    }

    public static String getClassValue(Instances inst, int i) {
        return getValue(inst, i, inst.numAttributes() - 1);
    }

    public Clusters classify(List<Tunnel> tunnels, LayeredTunnels layered,
            Clusters clustersOld) throws IOException {

        int count = 0;
        Instances data = load(cs_.getTrainingFile());
        Clusters clusters = new Clusters();

        try {
            Classifier cls = new weka.classifiers.lazy.IBk();

            Evaluation eval = new Evaluation(data);
            eval.crossValidateModel(cls, data, 2, new Random());


            Printer.println(eval.toSummaryString());

            Printer.println(eval.toMatrixString());

            Printer.println("Training on " + data.numInstances()
                    + " tunnels.", Printer.NORMAL);
            if ("knn".equals(cs_.getClassifier().toLowerCase())) {
                cls = new weka.classifiers.lazy.IBk();
            } else if ("svm".equals(cs_.getClassifier().toLowerCase())) {
                cls = new weka.classifiers.functions.SMO();
            } else if ("naive_bayes".equals(cs_.getClassifier().toLowerCase())) {
                cls = new weka.classifiers.bayes.NaiveBayes();
            } else {
                Logger.getLogger("caver").log(Level.WARNING, "Unknown classifier {0}, using k-means.", cs_.getClassifier());
                cls = new weka.classifiers.lazy.IBk();
            }

            cls.buildClassifier(data);

            Instance instance = new Instance(data.firstInstance());

            data.delete();
            Printer.println("Classifying " + tunnels.size() + " tunnels."
                    + Printer.NORMAL);
            Printer.println("Classifying " + layered.size() + " layered tunnels.");

            for (LayeredTunnel lt : layered) {

                int attIndex = 0;
                for (int i = 0; i < lt.size(); i++) {
                    instance.setValue(attIndex++, lt.getX(i));
                    instance.setValue(attIndex++, lt.getY(i));
                    instance.setValue(attIndex++, lt.getZ(i));
                }
                instance.setValue(attIndex++, lt.getTunnel().getLength());
                if (0 == lt.getTunnel().getLength()) {

                    Logger.getLogger("caver").log(Level.WARNING,
                            "Tunnel legnth 0 at classification.");
                }

                data.add(instance);
                instance.setDataset(data);

                instance.setClassMissing();

                double d = cls.classifyInstance(instance);

                String clusterS = instance.classAttribute().value(
                        (int) Math.round(d));
                int clusterN;
                if (!clusterS.equals("X")) {
                    clusterN = Integer.parseInt(clusterS);
                    clusters.addTunnel(ClusterId.create(clusterN), lt.getTunnel());
                } else {
                    if (cs_.generateUnclassifiedCluster()) {
                        clusters.addTunnel(ClusterId.create(999999), lt.getTunnel());
                    }

                    count++;
                }
            }

            for (Tunnel t : clustersOld.getTunnels()) {
                Cluster c = t.getCluster();
                // save only clusters with assigned classes in training
                if (cs_.getMinTrainingTunnels() <= c.size()
                        && c.getPriority() <= cs_.getMaxTrainingClusters()) {
                    clusters.addTunnel(c.getId(), t);
                }
            }

        } catch (Exception e) {
            throw new RuntimeException(e);

        }

        Printer.println(count + " tunnels classified as small clusters.");
        Printer.println("Clustered tunnels after classification "
                + clusters.getTunnels().size());

        return clusters;
    }

    public int getTunnelNumber() {
        return tunnels_.size();
    }
}
